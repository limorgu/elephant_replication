{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELEPHANT – (clean)\n",
    "\n",
    "**This notebook was cleaned for sharing:** all cell outputs and execution counts were removed.\n",
    "\n",
    "## How to upload to GitHub\n",
    "### Option A — Upload in the GitHub UI (fast)\n",
    "1. Open the repo: `https://github.com/limorgu/elephant_replication`\n",
    "2. Click **Add file → Upload files**\n",
    "3. Drag this `.ipynb` file in\n",
    "4. Click **Commit changes**\n",
    "\n",
    "### Option B — Push with git (recommended)\n",
    "```bash\n",
    "cd /path/to/elephant_replication\n",
    "mkdir -p notebooks\n",
    "mv elephant_10examples_clean.ipynb notebooks/\n",
    "git add notebooks/elephant_10examples_clean.ipynb\n",
    "git commit -m \"Add cleaned notebook\"\n",
    "git push\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bP3KKlxgnNjX"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pXJPpLcaBKp",
    "outputId": "b6d81931-eba2-4b3e-8e80-23ea72736ab6"
   },
   "outputs": [],
   "source": [
    "ls \"/content/drive/MyDrive/Colab Notebooks/elephant/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sm2g3EEkir21",
    "outputId": "44dbd181-fd12-4202-c0aa-2d018d3f4a39"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "he7YHQNNaH6S",
    "outputId": "0ecd2460-8cf4-4595-b049-4dcc54889da6"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6pAXOO9s-m3",
    "outputId": "0bce0912-ead5-439e-841a-49954cf6fcea"
   },
   "outputs": [],
   "source": [
    "cp -r \"/content/drive/MyDrive/Colab Notebooks/elephant/\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Rar2BZ6mcxV",
    "outputId": "c0534d06-8450-45b5-9689-d79055952f7b"
   },
   "outputs": [],
   "source": [
    "cd elphant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc47DXvgbEMz"
   },
   "source": [
    "Run Stage 0 - run the LLM to create responese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OL9fl7p-azrZ",
    "outputId": "1ea90573-f27f-4e08-b2ee-58b568a15072"
   },
   "outputs": [],
   "source": [
    "!python get_responses_models.py \\\n",
    "  --input_file ./sample_datasets/OEQ_sample.csv \\\n",
    "  --input_column prompt \\\n",
    "  --output_file ./output_data/OEQ_responses_openai.csv \\\n",
    "  --output_column get_responses_models.py \\\n",
    "  --model openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5d-WrQY_78Y",
    "outputId": "b6b2d489-0a48-4cb3-a207-b23fdced8a34"
   },
   "outputs": [],
   "source": [
    "!python get_responses_models.py \\\n",
    "--input_file ./sample_datasets/OEQ_sample.csv \\\n",
    "--input_column prompt \\\n",
    "--output_file ./output_data/OEQ_responses_grock2.csv \\\n",
    "--output_column grock_response \\\n",
    "--model grock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBJvA6rzNEgF",
    "outputId": "23165694-82ff-4a46-8a32-9e030368c405"
   },
   "outputs": [],
   "source": [
    "!python get_responses_models.py \\\n",
    "--input_file ./sample_datasets/OEQ_sample.csv \\\n",
    "--input_column prompt \\\n",
    "--output_file ./output_data/OEQ_responses_claude.csv \\\n",
    "--output_column claud_response \\\n",
    "--model claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFaCeM6C2EUm",
    "outputId": "94b41c14-78a7-4277-e3d9-349557fc663d"
   },
   "outputs": [],
   "source": [
    "!python get_responses_models.py \\\n",
    "--input_file ./sample_datasets/OEQ_sample.csv \\\n",
    "--input_column prompt \\\n",
    "--output_file ./output_data/OEQ_responses_grock.csv \\\n",
    "--output_column grock_response \\\n",
    "--model grock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCN20CZVnOFr",
    "outputId": "e7b6fc80-f996-4c3c-c651-ced84c6926d0"
   },
   "outputs": [],
   "source": [
    "# Side A: FLIP\n",
    "!python get_responses_models.py \\\n",
    "  --input_file ./sample_datasets/AITA-NTA-FLIP_sample.csv \\\n",
    "  --input_column prompt \\\n",
    "  --output_file ./output_data/AITA_NTA_FLIP_response.csv \\\n",
    "  --output_column openai_response \\\n",
    "  --model openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiFjc7644fCw"
   },
   "outputs": [],
   "source": [
    "# Side B: Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-FV_hW9eAQf"
   },
   "source": [
    "Stage 1 - Run the scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMxQY84fd77Q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWUkTQM3dyAQ",
    "outputId": "ae99928e-5bf7-46db-d63f-ba877f436af1"
   },
   "outputs": [],
   "source": [
    "!python sycophancy_scorers.py \\\n",
    "  --input_file /content/elephant/output_data/OEQ_responses_openai.csv \\\n",
    "  --prompt_column prompt \\\n",
    "  --response_column get_responses_models.py \\\n",
    "  --output_column_tag gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4ph2XMJEhMR",
    "outputId": "e0bad950-3528-42ef-e5d7-d7a3711e9ca0"
   },
   "outputs": [],
   "source": [
    "!python sycophancy_scorers.py \\\n",
    "  --input_file /content/elephant/output_data/OEQ_responses_grock2.csv \\\n",
    "  --prompt_column prompt \\\n",
    "  --response_column grock_response \\\n",
    "  --output_column_tag grock_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-u5VOT_JETKo",
    "outputId": "2e1e22d5-4a5a-4993-ebc1-3928b5d6fd0e"
   },
   "outputs": [],
   "source": [
    "!python sycophancy_scorers.py \\\n",
    "  --input_file /content/elephant/output_data/OEQ_responses_claude.csv \\\n",
    "  --prompt_column prompt \\\n",
    "  --response_column claud_response \\\n",
    "  --output_column_tag claude_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1HCPGrh2BUZ",
    "outputId": "903eb34f-7097-48d3-e5f4-9b2cdd956065"
   },
   "outputs": [],
   "source": [
    "#Step 1: Compute moral sycophancy for AITA\n",
    "\n",
    "!python sycophancy_scorers.py \\\n",
    "  --input_file /content/elphant/output_data/AITA_NTA_FLIP_response.csv \\\n",
    "  --prompt_column prompt \\\n",
    "  --response_column openai_response \\\n",
    "  --output_column_tag openai_fliped_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ylre8GhF4zPz",
    "outputId": "36cacaa9-596d-4738-a922-7fdb46d05949"
   },
   "outputs": [],
   "source": [
    "# score sycophancy\n",
    "!python sycophancy_scorers.py \\\n",
    "  --input_file ./output_data/OEQ_responses.csv \\\n",
    "  --prompt_column prompt \\\n",
    "  --response_column gpt_response \\\n",
    "  --output_column_tag gpt4o\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNaVQjT1eOwJ"
   },
   "source": [
    "Stage 2 - compare to human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ5AkW0UeCxs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLNyx72leN-W"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/elphant/output_data/OEQ_responses_2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1SS-VaeeZ-c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set(font_scale = 2.2)\n",
    "sns.set_style(\"white\", {\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.serif\": ['Helvetica'],\n",
    "    \"font.scale\": 2.2\n",
    "})\n",
    "sns.set_style(\"ticks\", {\"xtick.major.size\": 4,\n",
    "                        \"ytick.major.size\": 4})\n",
    "\n",
    "def apply_style(ax):\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    for yy in [0.2,0.4,0.6,0.8]: # change to wherever you want dashed lines\n",
    "        ax.axhline(y=yy, linestyle='--', color='black', linewidth=1, alpha=0.3)\n",
    "\n",
    "# Extract model and metric info from columns\n",
    "data = []\n",
    "for col in df:\n",
    "    for metric in ['validation', 'indirectness', 'framing']:\n",
    "        if metric in col:\n",
    "            model = col.replace(metric + '_', '')\n",
    "            values = pd.to_numeric(df[col], errors='coerce').dropna().astype(int).values\n",
    "            mean = values.mean()\n",
    "            std = 1.96*scipy.stats.sem(values)\n",
    "            data.append({'model': model, 'metric': metric, 'mean': mean, 'CI': std, 'col':col})\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "fZxCumiieq2p",
    "outputId": "71a764cf-c9f3-42b3-b3c2-4b5582714bd8"
   },
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame(data)\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "QmpDI87EevbS",
    "outputId": "0b7739e9-03e2-4b7f-f02c-5eee074420be"
   },
   "outputs": [],
   "source": [
    "metrics = [ 'validation',     'indirectness','framing']\n",
    "\n",
    "models = plot_df['model'].unique()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "\n",
    "# Plot grouped bars for each model within each metric\n",
    "for i, model in enumerate(models):\n",
    "    print(i)\n",
    "    model_df = plot_df[plot_df['model'] == model].set_index('metric').loc[metrics]\n",
    "\n",
    "#     ax.errorbar(df.Feature, model_df['mean'],m, linewidth=0, marker='o', ms=5,\n",
    "#                 elinewidth=1, color=color, alpha=0.7)\n",
    "    hatch = '\\\\' if i == 0 else None\n",
    "    ax.bar(\n",
    "    x + i*width,\n",
    "    model_df['mean'],\n",
    "    width,\n",
    "    yerr=model_df['CI'],\n",
    "    label=model,\n",
    "    hatch=hatch\n",
    "    )\n",
    "apply_style(ax)\n",
    "ax.set_xticks(x + width * (len(models) - 1) / 2)\n",
    "ax.set_xticklabels([x.capitalize() for x in metrics])\n",
    "ax.set_ylabel(\"Mean Score\")\n",
    "ax.set_title(\"ELEPHANT Metrics of Social Sycophancy on OEQ\")\n",
    "\n",
    "\n",
    "ax.legend(    bbox_to_anchor=(0.97, 1.05\n",
    "                             ),  # x shifted left from 1.01 → 0.95, y shifted up from 1 → 1.05\n",
    " loc='upper left', borderaxespad=0,fontsize=20)#columnspacing=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQe0gv7fezcL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# split out human baselines\n",
    "human = (\n",
    "    plot_df.query(\"model == 'human'\")[[\"metric\", \"mean\", \"CI\"]]\n",
    "    .rename(columns={\"mean\": \"mean_human\", \"CI\": \"CI_human\"})\n",
    ")\n",
    "# join back to non-human rows\n",
    "tmp = (\n",
    "    plot_df.query(\"model == 'gpt4o'\")\n",
    "    .merge(human, on=\"metric\", how=\"left\", validate=\"m:1\")\n",
    ")\n",
    "\n",
    "# compute difference and CI for the difference\n",
    "# CI -> SE assuming 95% CI: CI = 1.96 * SE\n",
    "z = 1.96\n",
    "se_model = tmp[\"CI\"] / z\n",
    "se_human = tmp[\"CI_human\"] / z\n",
    "se_diff = np.sqrt(se_model**2 + se_human**2)\n",
    "\n",
    "tmp[\"mean_diff\"] = tmp[\"mean\"] - tmp[\"mean_human\"]\n",
    "tmp[\"CI_diff\"] = z * se_diff\n",
    "tmp[\"lower\"] = tmp[\"mean_diff\"] - tmp[\"CI_diff\"]\n",
    "tmp[\"upper\"] = tmp[\"mean_diff\"] + tmp[\"CI_diff\"]\n",
    "\n",
    "# final rate\n",
    "final_df = tmp[[\n",
    "    \"model\", \"metric\", \"mean_diff\", \"CI_diff\", \"lower\", \"upper\",\n",
    "    \"mean\", \"CI\", \"mean_human\", \"CI_human\", \"col\"  # keep extras if useful\n",
    "]].sort_values([\"model\", \"metric\"]).reset_index(drop=True)\n",
    "\n",
    "for _, r in final_df.iterrows():\n",
    "    print(f\"{r['model']:>8} | {r['metric']:<13} \"\n",
    "          f\"Δ={r['mean_diff']:.2%} ± {r['CI_diff']:.2%} \"\n",
    "          f\"[{r['lower']:.2%}, {r['upper']:.2%}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Dun9rze6yR"
   },
   "outputs": [],
   "source": [
    "# Compare AITA to Human\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Collect means & CIs per metric per model\n",
    "# -----------------------------\n",
    "data = []\n",
    "\n",
    "for col in df.columns:\n",
    "    for metric in ['validation', 'indirectness', 'framing']:\n",
    "        if col.startswith(metric + \"_\"):\n",
    "            # model name is whatever comes after \"metric_\"\n",
    "            model = col.replace(metric + \"_\", \"\")  # e.g. \"validation_gpt4o\" -> \"gpt4o\"\n",
    "\n",
    "            values = (\n",
    "                pd.to_numeric(df[col], errors='coerce')\n",
    "                .dropna()\n",
    "                .astype(int)\n",
    "                .values\n",
    "            )\n",
    "            if len(values) == 0:\n",
    "                continue\n",
    "\n",
    "            mean = values.mean()\n",
    "            ci = 1.96 * stats.sem(values)  # 95% CI\n",
    "\n",
    "            data.append({\n",
    "                \"model\": model,\n",
    "                \"metric\": metric,\n",
    "                \"mean\": mean,\n",
    "                \"CI\": ci,\n",
    "                \"col\": col,\n",
    "            })\n",
    "            break  # stop checking other metrics once matched\n",
    "\n",
    "plot_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTfsb9Z15_Hz"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Compare a specific model to human\n",
    "# -----------------------------\n",
    "model_to_compare = \"gpt4o\"  # <- change if you have a different model tag\n",
    "\n",
    "# human baseline per metric\n",
    "human = (\n",
    "    plot_df.query(\"model == 'human'\")[[\"metric\", \"mean\", \"CI\"]]\n",
    "    .rename(columns={\"mean\": \"mean_human\", \"CI\": \"CI_human\"})\n",
    ")\n",
    "\n",
    "# rows for the chosen model\n",
    "tmp = (\n",
    "    plot_df.query(\"model == @model_to_compare\")\n",
    "    .merge(human, on=\"metric\", how=\"left\", validate=\"m:1\")\n",
    ")\n",
    "\n",
    "# compute Δ (model − human) and CI for the difference\n",
    "z = 1.96\n",
    "se_model = tmp[\"CI\"] / z\n",
    "se_human = tmp[\"CI_human\"] / z\n",
    "se_diff = np.sqrt(se_model**2 + se_human**2)\n",
    "\n",
    "tmp[\"mean_diff\"] = tmp[\"mean\"] - tmp[\"mean_human\"]\n",
    "tmp[\"CI_diff\"] = z * se_diff\n",
    "tmp[\"lower\"] = tmp[\"mean_diff\"] - tmp[\"CI_diff\"]\n",
    "tmp[\"upper\"] = tmp[\"mean_diff\"] + tmp[\"CI_diff\"]\n",
    "\n",
    "final_df = tmp[[\n",
    "    \"model\", \"metric\", \"mean_diff\", \"CI_diff\", \"lower\", \"upper\",\n",
    "    \"mean\", \"CI\", \"mean_human\", \"CI_human\", \"col\"\n",
    "]].sort_values([\"metric\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXdTUYig6KoR"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 3. Print numeric summary\n",
    "# -----------------------------\n",
    "for _, r in final_df.iterrows():\n",
    "    print(\n",
    "        f\"{r['model']:>8} | {r['metric']:<13} \"\n",
    "        f\"Δ={r['mean_diff']:.2%} ± {r['CI_diff']:.2%} \"\n",
    "        f\"[{r['lower']:.2%}, {r['upper']:.2%}]\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leG44bLRFTUY"
   },
   "outputs": [],
   "source": [
    "#compare model's score\n",
    "import pandas as pd\n",
    "gpt_scores   = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/elphant/outputs/OEQ_responses_openai_elephant_scored.csv')\n",
    "grock_scores = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/elphant/outputs/OEQ_responses_grock2_elephant_scored.csv')\n",
    "claude_scores = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/elphant/outputs/OEQ_responses_claude_elephant_scored.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7Zz3yP4F425"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7OVbh-M6VMC",
    "outputId": "4b4b7e9e-ec79-4a86-8762-7cc512daf272"
   },
   "outputs": [],
   "source": [
    "claude_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "1DzO5pPZFr1K",
    "outputId": "df276682-03b5-44b0-ef16-2dc0d6bb1480"
   },
   "outputs": [],
   "source": [
    "grock_scores[['validation_grock_score', 'indirectness_grock_score','framing_grock_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "fjkTjlbOF1Si",
    "outputId": "75e19481-c4fe-495d-cbe1-ca3da4904867"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "mean_score_gpt = gpt_scores[['validation_gpt4o', 'indirectness_gpt4o',\n",
    "       'framing_gpt4o']].apply(pd.to_numeric, errors='coerce').mean()\n",
    "mean_score_grock = grock_scores[['validation_grock_score', 'indirectness_grock_score','framing_grock_score']].apply(pd.to_numeric, errors='coerce').mean()\n",
    "mean_score_claude = claude_scores[['validation_claude_score',\n",
    "       'indirectness_claude_score', 'framing_claude_score']].apply(pd.to_numeric, errors='coerce').mean()\n",
    "\n",
    "# Create tidy (long-form) dataframe for seaborn\n",
    "df_plot = pd.DataFrame({\n",
    "    'score_type': ['Validation', 'Indirectness', 'Framing'],\n",
    "    'GPT': mean_score_gpt.values,\n",
    "    'Grock': mean_score_grock.values,\n",
    "    'claude': mean_score_claude.values\n",
    "})\n",
    "\n",
    "# Melt into seaborn-friendly format\n",
    "df_long = df_plot.melt(id_vars='score_type', var_name='model', value_name='mean_score')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df_long, x='score_type', y='mean_score', hue='model', palette=\"viridis\")\n",
    "\n",
    "plt.title(\"Mean Scores per Model\")\n",
    "plt.xlabel(\"Score Type\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTLC11P9lxUc"
   },
   "outputs": [],
   "source": [
    "fliped_openai_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/elphant/outputs/AITA_NTA_FLIP_response_elephant_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgGDpjIml43v"
   },
   "outputs": [],
   "source": [
    "mean_score_fliped_open_ai = fliped_openai_df[['validation_openai_fliped_score','indirectness_openai_fliped_score','framing_openai_fliped_score']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "id": "QVTnJFvnmTuy",
    "outputId": "7a47f386-9389-4010-abcf-9e384e1ad6a3"
   },
   "outputs": [],
   "source": [
    "mean_score_fliped_open_ai.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxTHEn2Oq0id",
    "outputId": "f089e549-ac5f-4472-a039-2c0366d8a4f9"
   },
   "outputs": [],
   "source": [
    "cp -r   \"/content/drive/MyDrive/Colab Notebooks/elphant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gFbYilhQrVCT",
    "outputId": "7d0b6242-4cea-4cb9-f0e0-015ce1d5788a"
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  }
 ],
 "metadata": {
  "cleaned_at_utc": "2026-01-08T14:35:07Z",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
